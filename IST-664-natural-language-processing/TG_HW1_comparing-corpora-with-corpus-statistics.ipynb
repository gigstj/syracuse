{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "text_mobydick = nltk.corpus.gutenberg.raw(\"melville-moby_dick.txt\")\n",
    "text_hamlet = nltk.corpus.gutenberg.raw(\"shakespeare-hamlet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "\n",
    "tokens_mobydick = nltk.word_tokenize(text_mobydick)\n",
    "tokens_hamlet = nltk.word_tokenize(text_hamlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokens all lowercase\n",
    "\n",
    "tokens_mobydick_lower = [w.lower() for w in tokens_mobydick]\n",
    "tokens_hamlet_lower = [w.lower() for w in tokens_hamlet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stopwords to the tokens\n",
    "\n",
    "nltk_stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "more_stop_words = [\"'d\", \"'s\", \"'s\", \"n't\", \"--\", \"''\", \"``\"]\n",
    "all_stop_words = nltk_stop_words + more_stop_words + list(string.punctuation)\n",
    "\n",
    "# Apply the stopwords\n",
    "\n",
    "tokens_mobydick_lower_stopped = [w for w in tokens_mobydick_lower if not w in all_stop_words]\n",
    "tokens_hamlet_lower_stopped = [w for w in tokens_hamlet_lower if not w in all_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem with the Porter stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer_porter = PorterStemmer()\n",
    "\n",
    "# Perform the stemming\n",
    "\n",
    "tokens_mobydick_stemmed = [stemmer_porter.stem(w) for w in tokens_mobydick_lower_stopped]\n",
    "tokens_hamlet_stemmed = [stemmer_porter.stem(w) for w in tokens_hamlet_lower_stopped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create token frequency distributions\n",
    "\n",
    "fdist_tokens_mobydick = nltk.FreqDist(tokens_mobydick_stemmed)\n",
    "fdist_tokens_hamlet = nltk.FreqDist(tokens_hamlet_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50 words comparison dataframe\n",
    "\n",
    "top_50_tokens_mobydick = pd.DataFrame(fdist_tokens_mobydick.most_common(50), columns = [\"Token\", \"Count\"])\n",
    "top_50_tokens_hamlet = pd.DataFrame(fdist_tokens_hamlet.most_common(50), columns = [\"Token\", \"Count\"])\n",
    "\n",
    "# Add a column for the percent of total\n",
    "\n",
    "top_50_tokens_mobydick[\"PctTotal\"] = top_50_tokens_mobydick[\"Count\"] / len(tokens_mobydick_stemmed)\n",
    "top_50_tokens_hamlet[\"PctTotal\"] = top_50_tokens_hamlet[\"Count\"] / len(tokens_hamlet_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of bigrams\n",
    "\n",
    "bigrams_mobydick = list(nltk.bigrams(tokens_mobydick_lower_stopped))\n",
    "bigrams_hamlet = list(nltk.bigrams(tokens_hamlet_lower_stopped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigram frequency distribution (normalized frequency)\n",
    "\n",
    "from nltk import BigramCollocationFinder\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# Initiate the finders\n",
    "\n",
    "finder_mobydick = BigramCollocationFinder.from_words(tokens_mobydick_lower_stopped)\n",
    "finder_hamlet = BigramCollocationFinder.from_words(tokens_hamlet_lower_stopped)\n",
    "\n",
    "# Score based on normalized frequency\n",
    "\n",
    "scored_mobydick_freq = finder_mobydick.score_ngrams(bigram_measures.raw_freq)\n",
    "scored_hamlet_freq = finder_hamlet.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "# Top 50 bigrams comparison dataframe\n",
    "\n",
    "top_50_bigrams_mobydick_freq = pd.DataFrame(scored_mobydick_freq[0:49], columns = [\"Bigram\", \"PctTotal\"])\n",
    "top_50_bigrams_hamlet_freq = pd.DataFrame(scored_hamlet_freq[0:49], columns = [\"Bigram\", \"PctTotal\"])\n",
    "\n",
    "# Add a column for the count\n",
    "\n",
    "top_50_bigrams_mobydick_freq[\"Count\"] = round(top_50_bigrams_mobydick_freq[\"PctTotal\"] * len(bigrams_mobydick))\n",
    "top_50_bigrams_hamlet_freq[\"Count\"] = round(top_50_bigrams_hamlet_freq[\"PctTotal\"] * len(bigrams_hamlet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigram frequency distribution (mutual information)\n",
    "\n",
    "from nltk import BigramCollocationFinder\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# Add freq filters on the finders\n",
    "\n",
    "finder_mobydick.apply_freq_filter(5)\n",
    "finder_hamlet.apply_freq_filter(5)\n",
    "\n",
    "# Score based on mutual information\n",
    "\n",
    "scored_mobydick_pmi = finder_mobydick.score_ngrams(bigram_measures.pmi)\n",
    "scored_hamlet_pmi = finder_hamlet.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "# Top 50 bigrams comparison dataframe\n",
    "\n",
    "top_50_bigrams_mobydick_pmi = pd.DataFrame(scored_mobydick_pmi[0:49], columns = [\"Bigram\", \"PMIScore\"])\n",
    "top_50_bigrams_hamlet_pmi = pd.DataFrame(scored_hamlet_pmi[0:49], columns = [\"Bigram\", \"PMIScore\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of Trigrams\n",
    "\n",
    "trigrams_mobydick = list(nltk.trigrams(tokens_mobydick_lower_stopped))\n",
    "trigrams_hamlet = list(nltk.trigrams(tokens_hamlet_lower_stopped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trigram frequency distribution (normalized frequency)\n",
    "\n",
    "from nltk import TrigramAssocMeasures\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Initiate the finders\n",
    "\n",
    "finder_mobydick = TrigramCollocationFinder.from_words(tokens_mobydick_lower_stopped)\n",
    "finder_hamlet = TrigramCollocationFinder.from_words(tokens_hamlet_lower_stopped)\n",
    "\n",
    "# Score based on normalized frequency\n",
    "\n",
    "scored_mobydick_freq = finder_mobydick.score_ngrams(trigram_measures.raw_freq)\n",
    "scored_hamlet_freq = finder_hamlet.score_ngrams(trigram_measures.raw_freq)\n",
    "\n",
    "# Top 50 bigrams comparison dataframe\n",
    "\n",
    "top_50_trigrams_mobydick_freq = pd.DataFrame(scored_mobydick_freq[0:49], columns = [\"Trigram\", \"PctTotal\"])\n",
    "top_50_trigrams_hamlet_freq = pd.DataFrame(scored_hamlet_freq[0:49], columns = [\"Trigram\", \"PctTotal\"])\n",
    "\n",
    "# Add a column for the count\n",
    "\n",
    "top_50_trigrams_mobydick_freq[\"Count\"] = round(top_50_trigrams_mobydick_freq[\"PctTotal\"] * len(trigrams_mobydick))\n",
    "top_50_trigrams_hamlet_freq[\"Count\"] = round(top_50_trigrams_hamlet_freq[\"PctTotal\"] * len(trigrams_hamlet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the top 50 dataframes into csv files\n",
    "\n",
    "top_50_tokens_mobydick.to_csv(\"top_50_tokens_mobydick.csv\")\n",
    "top_50_tokens_hamlet.to_csv(\"top_50_tokens_hamlet.csv\")\n",
    "\n",
    "top_50_bigrams_mobydick_freq.to_csv(\"top_50_bigrams_mobydick_freq.csv\")\n",
    "top_50_bigrams_hamlet_freq.to_csv(\"top_50_bigrams_hamlet_freq.csv\")\n",
    "\n",
    "top_50_bigrams_mobydick_pmi.to_csv(\"top_50_bigrams_mobydick_pmi.csv\")\n",
    "top_50_bigrams_hamlet_pmi.to_csv(\"top_50_bigrams_hamlet_pmi.csv\")\n",
    "\n",
    "top_50_trigrams_mobydick_freq.to_csv(\"top_50_trigrams_mobydick_freq.csv\")\n",
    "top_50_trigrams_hamlet_freq.to_csv(\"top_50_trigrams_hamlet_freq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Text\n",
      "255028\n",
      "20742\n",
      "36372\n",
      "5535\n",
      "\n",
      " All Lowercase\n",
      "255028\n",
      "18701\n",
      "36372\n",
      "4807\n",
      "\n",
      " Removed Stopwords\n",
      "108649\n",
      "18543\n",
      "15803\n",
      "4685\n",
      "\n",
      " Stemmed\n",
      "108649\n",
      "12337\n",
      "15803\n",
      "3780\n"
     ]
    }
   ],
   "source": [
    "# Summary of vocabulary size at different stages of text processing\n",
    "\n",
    "# Raw text\n",
    "\n",
    "print(\"Raw Text\")\n",
    "print(len(tokens_mobydick))\n",
    "print(len(set(tokens_mobydick)))\n",
    "print(len(tokens_hamlet))\n",
    "print(len(set(tokens_hamlet)))\n",
    "\n",
    "# All Lowercase\n",
    "\n",
    "print(\"\\n All Lowercase\")\n",
    "print(len(tokens_mobydick_lower))\n",
    "print(len(set(tokens_mobydick_lower)))\n",
    "print(len(tokens_hamlet_lower))\n",
    "print(len(set(tokens_hamlet_lower)))\n",
    "\n",
    "# Removed stopwords\n",
    "\n",
    "print(\"\\n Removed Stopwords\")\n",
    "print(len(tokens_mobydick_lower_stopped))\n",
    "print(len(set(tokens_mobydick_lower_stopped)))\n",
    "print(len(tokens_hamlet_lower_stopped))\n",
    "print(len(set(tokens_hamlet_lower_stopped)))\n",
    "\n",
    "# Stemmed with porter stemmer\n",
    "\n",
    "print(\"\\n Stemmed\")\n",
    "print(len(tokens_mobydick_stemmed))\n",
    "print(len(set(tokens_mobydick_stemmed)))\n",
    "print(len(tokens_hamlet_stemmed))\n",
    "print(len(set(tokens_hamlet_stemmed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09839938289460998\n",
      "-0.13152664859981933\n"
     ]
    }
   ],
   "source": [
    "print((18701-20742) / 20742)\n",
    "print((4807-5535) / 5535)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5739722697115611\n",
      "-0.5655174309908721\n"
     ]
    }
   ],
   "source": [
    "print((108649-255028) / 255028)\n",
    "print((15803 - 36372) / 36372)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "print(4807-4685)\n",
    "print(18701-18543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.33468155098959174\n",
      "-0.19316969050160085\n"
     ]
    }
   ],
   "source": [
    "print((12337-18543) / 18543)\n",
    "print((3780-4685) / 4685)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00037642925482692097"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_mobydick_lower.count(\"shall\") / len(tokens_mobydick_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018695699989002528\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "print(tokens_hamlet_lower.count(\"must\") / len(tokens_hamlet_lower))\n",
    "print(tokens_hamlet_lower.count(\"would\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([t in trigrams_mobydick for t in trigrams_hamlet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "\n",
    "**Different tokenizers in the NLTK package**\n",
    "https://www.kite.com/python/docs/nltk.word_tokenize\n",
    "https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5\n",
    "\n",
    "**Differences between Porter stemmer and Lancaster stemmer**\n",
    "https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\n",
    "\n",
    "**About the Gutenberg Collection**\n",
    "https://www.gutenberg.org/about/background/history_and_philosophy.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "837a5e7ae28b4e198c1d8cf4796a058d56e0b65e3e1483208ccae61f9a209f4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
