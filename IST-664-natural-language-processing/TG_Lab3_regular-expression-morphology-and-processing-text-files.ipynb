{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "### **Lab**\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text file path\n",
    "\n",
    "filepath = \"crimeandpunishment.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex function to retrieve \n",
    "\n",
    "def alpha_filter(w):\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file, read the text, and close it\n",
    "\n",
    "f = open(filepath, 'r')\n",
    "filetext = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by the regular word tokenizer\n",
    "\n",
    "filetokens = nltk.word_tokenize(filetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all tokens lowercase\n",
    "\n",
    "filewords = [w.lower() for w in filetokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display first 50 words from file:\n",
      "['produced', 'by', 'john', 'bickers', ';', 'and', 'dagny', 'crime', 'and', 'punishment', 'by', 'fyodor', 'dostoevsky', 'translated', 'by', 'constance', 'garnett', 'translator', \"'s\", 'preface', 'a', 'few', 'words', 'about', 'dostoevsky', 'himself', 'may', 'help', 'the', 'english', 'reader', 'to', 'understand', 'his', 'work', '.', 'dostoevsky', 'was', 'the', 'son', 'of', 'a', 'doctor', '.', 'his', 'parents', 'were', 'very', 'hard-working', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Display the first 50 words\n",
    "\n",
    "print (\"Display first 50 words from file:\")\n",
    "print (filewords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the stop word file\n",
    "\n",
    "fstop = open('Smart.English.stop', 'r')\n",
    "stoptext = fstop.read()\n",
    "fstop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the stop word file\n",
    "\n",
    "stopwords = nltk.word_tokenize(stoptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display first 50 Stopwords:\n",
      "['â€™s', 'a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 50 stop words\n",
    "\n",
    "print (\"Display first 50 Stopwords:\")\n",
    "print (stopwords[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to process bigrams\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(filewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose to use both the non-alpha word filter and a stopwords filter\n",
    "\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "finder.apply_word_filter(lambda w: w in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Bigrams from file with top 50 frequencies\n",
      "\n",
      "(('katerina', 'ivanovna'), 0.0008468990312593629)\n",
      "(('pyotr', 'petrovitch'), 0.000683111954459203)\n",
      "(('wo', \"n't\"), 0.0004913612304004793)\n",
      "(('ca', \"n't\"), 0.00048736642364925596)\n",
      "(('pulcheria', 'alexandrovna'), 0.00048337161689803257)\n",
      "(('avdotya', 'romanovna'), 0.0004594027763906921)\n",
      "(('rodion', 'romanovitch'), 0.0003435533806052132)\n",
      "(('porfiry', 'petrovitch'), 0.00032357934684909616)\n",
      "(('marfa', 'petrovna'), 0.00030760011984420254)\n",
      "(('sofya', 'semyonovna'), 0.0002836312793368621)\n",
      "(('raskolnikov', \"'s\"), 0.00021971437131728752)\n",
      "(('amalia', 'ivanovna'), 0.0002157195645660641)\n",
      "(('young', 'man'), 0.0002077299510636173)\n",
      "(('great', 'deal'), 0.00018775591730750026)\n",
      "((\"n't\", 'understand'), 0.00013981823629281934)\n",
      "(('ilya', 'petrovitch'), 0.0001318286227903725)\n",
      "(('ivanovna', \"'s\"), 0.0001238390092879257)\n",
      "(('sonia', \"'s\"), 0.00011584939578547888)\n",
      "(('make', 'haste'), 0.00010785978228303205)\n",
      "(('good', 'heavens'), 0.00010386497553180865)\n"
     ]
    }
   ],
   "source": [
    "# Score by frequency and display the top 50 bigrams\n",
    "\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "print (\"\\n Bigrams from file with top 50 frequencies\\n\")\n",
    "for item in scored[:20]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use frequently occurring words in mutual information\n",
    "\n",
    "finder.apply_freq_filter(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bigrams from file with top 50 mutual information scores\n",
      "\n",
      "(('praskovya', 'pavlovna'), 14.763517853413216)\n",
      "(('palais', 'de'), 14.34848035413437)\n",
      "(('de', 'cristal'), 14.348480354134367)\n",
      "(('explosive', 'lieutenant'), 14.248944680583456)\n",
      "(('semyon', 'zaharovitch'), 14.248944680583456)\n",
      "(('assistant', 'superintendent'), 13.91107504182707)\n",
      "(('arkady', 'ivanovitch'), 13.763517853413216)\n",
      "(('madame', 'resslich'), 13.567120640609708)\n",
      "(('afanasy', 'ivanovitch'), 13.34848035413437)\n",
      "(('nikodim', 'fomitch'), 13.348480354134368)\n",
      "(('andrey', 'semyonovitch'), 13.348480354134367)\n",
      "(('madame', 'lippevechsel'), 13.348480354134367)\n",
      "(('examining', 'lawyer'), 13.026552259247005)\n",
      "(('flushed', 'crimson'), 12.915520946858262)\n",
      "(('hay', 'market'), 12.911075041827068)\n",
      "(('chapter', 'iii'), 12.389122338631717)\n",
      "(('chapter', 'iv'), 12.389122338631717)\n",
      "(('dmitri', 'prokofitch'), 12.348480354134368)\n",
      "(('chapter', 'vi'), 12.348480354134367)\n",
      "(('canal', 'bank'), 12.32200814277318)\n"
     ]
    }
   ],
   "source": [
    "# Score by PMI and display the top 50 bigrams\n",
    "\n",
    "scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "print (\"\\nBigrams from file with top 50 mutual information scores\\n\")\n",
    "for item in scored[:20]:\n",
    "        print (item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "### **3.6 Stemming Activity**\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text file path\n",
    "\n",
    "filepath = \"desert.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file, read the text, and close it\n",
    "\n",
    "f = open(filepath, 'r')\n",
    "filetext = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize by the regular word tokenizer\n",
    "\n",
    "filetokens = nltk.word_tokenize(filetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all tokens lowercase\n",
    "\n",
    "filewords = [w.lower() for w in filetokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate stemmers\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the words\n",
    "\n",
    "filewords_ps_stemmed = [ps.stem(w) for w in filewords]\n",
    "filewords_ls_stemmed = [ls.stem(w) for w in filewords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521\n"
     ]
    }
   ],
   "source": [
    "# Get a random integer\n",
    "\n",
    "randint = np.random.randint(0, len(filewords))\n",
    "print(randint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabian\n",
      "arabian\n",
      "arab\n"
     ]
    }
   ],
   "source": [
    "# Print the stemmed word for that integer\n",
    "\n",
    "print(filewords[randint])\n",
    "print(filewords_ps_stemmed[randint])\n",
    "print(filewords_ls_stemmed[randint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "### **3.7 Regex Tokenization**\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK regex tokenization example\n",
    "\n",
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "pattern = r'''(?x)         # set flag to allow verbose regexps\n",
    "    (?:[A-Z]\\.)+           # abbreviations, e.g. U.S.A.\n",
    "    | \\w+(?:-\\w+)*         # words with optional internal hyphens\n",
    "    | \\$?\\d+(?:\\.\\d+)?%?   # currency and percentages, e.g. $12.40, 82%\n",
    "    | \\.\\.\\.               # ellipsis\n",
    "    | [][.,;\"'?():-_`]     # these are separate tokens; includes ], [\n",
    "    '''\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "837a5e7ae28b4e198c1d8cf4796a058d56e0b65e3e1483208ccae61f9a209f4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
