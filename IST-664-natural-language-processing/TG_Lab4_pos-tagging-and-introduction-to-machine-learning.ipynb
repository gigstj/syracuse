{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data\n",
    "\n",
    "from nltk.corpus import treebank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100676\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "# Store the tagged words\n",
    "\n",
    "treebank_tagged_words = treebank.tagged_words()\n",
    "print(len(treebank_tagged_words))\n",
    "print(treebank_tagged_words[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "dict_keys(['N', ',', 'C', 'J', 'M', 'V', 'D', 'I', '.', '-', 'R', 'T', 'P', 'W', '`', 'E', \"'\", ':', '$', 'F', 'U', 'S', 'L', '#'])\n"
     ]
    }
   ],
   "source": [
    "# Get classes of tags\n",
    "\n",
    "tag_classes_fd = nltk.FreqDist(tag[0] for (word, tag) in treebank_tagged_words)\n",
    "print(len(tag_classes_fd))\n",
    "print(tag_classes_fd.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N 28867\n",
      "V 12637\n",
      "I 9857\n",
      "D 8165\n",
      "- 6838\n",
      "J 6397\n",
      "C 5811\n",
      ", 4886\n",
      ". 3874\n",
      "P 3333\n",
      "R 3209\n",
      "T 2179\n",
      "M 927\n",
      "W 878\n",
      "$ 724\n",
      "` 712\n",
      "' 694\n",
      ": 563\n",
      "E 88\n",
      "# 16\n",
      "L 13\n",
      "F 4\n",
      "U 3\n",
      "S 1\n"
     ]
    }
   ],
   "source": [
    "# Distribution of tags\n",
    "\n",
    "for tag, freq in tag_classes_fd.most_common():\n",
    "    print(tag, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100676\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD')]\n"
     ]
    }
   ],
   "source": [
    "# Store the tagged sents\n",
    "\n",
    "treebank_tagged = treebank.tagged_sents()\n",
    "print(len(treebank_tagged_words))\n",
    "print(treebank_tagged_words[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'Vinken']\n"
     ]
    }
   ],
   "source": [
    "# Get the treebank tokens\n",
    "\n",
    "treebank_tokens = treebank.words()\n",
    "print(treebank_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3522\n"
     ]
    }
   ],
   "source": [
    "# Seperate test and train\n",
    "\n",
    "size = int(len(treebank_tagged) * 0.9)\n",
    "treebank_train = treebank_tagged[:size]\n",
    "treebank_test = treebank_tagged[size:]\n",
    "print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NN'), ('Vinken', 'NN'), (',', 'NN'), ('61', 'NN'), ('years', 'NN'), ('old', 'NN'), (',', 'NN'), ('will', 'NN'), ('join', 'NN'), ('the', 'NN'), ('board', 'NN'), ('as', 'NN'), ('a', 'NN'), ('nonexecutive', 'NN'), ('director', 'NN'), ('Nov.', 'NN'), ('29', 'NN'), ('.', 'NN'), ('Mr.', 'NN'), ('Vinken', 'NN'), ('is', 'NN'), ('chairman', 'NN'), ('of', 'NN'), ('Elsevier', 'NN'), ('N.V.', 'NN'), (',', 'NN'), ('the', 'NN'), ('Dutch', 'NN'), ('publishing', 'NN'), ('group', 'NN'), ('.', 'NN'), ('Rudolph', 'NN'), ('Agnew', 'NN'), (',', 'NN'), ('55', 'NN'), ('years', 'NN'), ('old', 'NN'), ('and', 'NN'), ('former', 'NN'), ('chairman', 'NN'), ('of', 'NN'), ('Consolidated', 'NN'), ('Gold', 'NN'), ('Fields', 'NN'), ('PLC', 'NN'), (',', 'NN'), ('was', 'NN'), ('named', 'NN'), ('*-1', 'NN'), ('a', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Start off with a default tagger\n",
    "\n",
    "t0 = nltk.DefaultTagger(\"NN\")\n",
    "print(t0.tag(treebank_tokens[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14697201017811704"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run evaluation on the test set\n",
    "\n",
    "t0.evaluate(treebank_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.'), ('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'JJ'), ('publishing', 'NN'), ('group', 'NN'), ('.', '.'), ('Rudolph', 'NNP'), ('Agnew', 'NNP'), (',', ','), ('55', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('and', 'CC'), ('former', 'JJ'), ('chairman', 'NN'), ('of', 'IN'), ('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP'), ('PLC', 'NNP'), (',', ','), ('was', 'VBD'), ('named', 'VBN'), ('*-1', '-NONE-'), ('a', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "# Train a unigram tagger\n",
    "\n",
    "t1 = nltk.UnigramTagger(treebank_tagged)\n",
    "print(t1.tag(treebank_tokens[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the unigram tagger\n",
    "\n",
    "print(t1.evaluate(treebank_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram tagger with backoffs\n",
    "\n",
    "t0 = nltk.DefaultTagger(\"NN\")\n",
    "t1 = nltk.UnigramTagger(treebank_train, backoff = t0)\n",
    "t2 = nltk.BigramTagger(treebank_train, backoff = t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8905852417302799\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test data\n",
    "\n",
    "print(t2.evaluate(treebank_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bigram tagger on some new text\n",
    "\n",
    "text = ''' Three Calgarians have found a rather unusual way of leaving snow and ice behind. They set off this week on foot and by camels on a grueling trek across the burning Arabian desert.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Three Calgarians have found a rather unusual way of leaving snow and ice behind.', 'They set off this week on foot and by camels on a grueling trek across the burning Arabian desert.']\n"
     ]
    }
   ],
   "source": [
    "# Split into sentences\n",
    "\n",
    "textsplit = nltk.sent_tokenize(text)\n",
    "print(textsplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Three', 'Calgarians', 'have', 'found', 'a', 'rather', 'unusual', 'way', 'of', 'leaving', 'snow', 'and', 'ice', 'behind', '.'], ['They', 'set', 'off', 'this', 'week', 'on', 'foot', 'and', 'by', 'camels', 'on', 'a', 'grueling', 'trek', 'across', 'the', 'burning', 'Arabian', 'desert', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Apply the word tokenizer to each sentence\n",
    "\n",
    "tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "print(tokentext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Three', 'CD'), ('Calgarians', 'NN'), ('have', 'VBP'), ('found', 'VBN'), ('a', 'DT'), ('rather', 'RB'), ('unusual', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('leaving', 'VBG'), ('snow', 'NN'), ('and', 'CC'), ('ice', 'NN'), ('behind', 'IN'), ('.', '.')], [('They', 'PRP'), ('set', 'VBN'), ('off', 'RP'), ('this', 'DT'), ('week', 'NN'), ('on', 'IN'), ('foot', 'NN'), ('and', 'CC'), ('by', 'IN'), ('camels', 'NN'), ('on', 'IN'), ('a', 'DT'), ('grueling', 'NN'), ('trek', 'NN'), ('across', 'IN'), ('the', 'DT'), ('burning', 'NN'), ('Arabian', 'NN'), ('desert', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Apply the bigram tagger to tag each sentence tokens\n",
    "\n",
    "taggedtext = [t2.tag(tokens) for tokens in tokentext]\n",
    "print(taggedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Three', 'CD'), ('Calgarians', 'NNPS'), ('have', 'VBP'), ('found', 'VBN'), ('a', 'DT'), ('rather', 'RB'), ('unusual', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('leaving', 'VBG'), ('snow', 'NN'), ('and', 'CC'), ('ice', 'NN'), ('behind', 'NN'), ('.', '.')], [('They', 'PRP'), ('set', 'VBD'), ('off', 'RP'), ('this', 'DT'), ('week', 'NN'), ('on', 'IN'), ('foot', 'NN'), ('and', 'CC'), ('by', 'IN'), ('camels', 'NNS'), ('on', 'IN'), ('a', 'DT'), ('grueling', 'NN'), ('trek', 'NN'), ('across', 'IN'), ('the', 'DT'), ('burning', 'NN'), ('Arabian', 'JJ'), ('desert', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Use the stanford POS tagger to tag each sentence tokens\n",
    "\n",
    "taggedtextStanford = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "print(taggedtextStanford)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Three', 'CD'), ('Calgarians', 'NN'), ('have', 'VBP'), ('found', 'VBN'), ('a', 'DT'), ('rather', 'RB'), ('unusual', 'JJ'), ('way', 'NN'), ('of', 'IN'), ('leaving', 'VBG'), ('snow', 'NN'), ('and', 'CC'), ('ice', 'NN'), ('behind', 'IN'), ('.', '.'), ('They', 'PRP'), ('set', 'VBN'), ('off', 'RP'), ('this', 'DT'), ('week', 'NN'), ('on', 'IN'), ('foot', 'NN'), ('and', 'CC'), ('by', 'IN'), ('camels', 'NN'), ('on', 'IN'), ('a', 'DT'), ('grueling', 'NN'), ('trek', 'NN'), ('across', 'IN'), ('the', 'DT'), ('burning', 'NN'), ('Arabian', 'NN'), ('desert', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of tagged tokens from a nested list to a list\n",
    "\n",
    "taggedtextflat = [pair for sent in taggedtext for pair in sent]\n",
    "print(taggedtextflat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "### **4.6.6 Running a POS Tagger on a Large Text and Looking at Tag Frequencies**\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the asuten-emma text from gutenberg corpus\n",
    "\n",
    "filename = nltk.corpus.gutenberg.fileids()[0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(filename)\n",
    "\n",
    "# Tokenize and POS tag the text with Stanford tagger\n",
    "\n",
    "emmatokens = nltk.word_tokenize(emmatext)\n",
    "emmatagged = nltk.pos_tag(emmatokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N 32045\n",
      "V 31349\n",
      "P 21611\n",
      "I 17899\n",
      "R 13926\n",
      "D 12643\n",
      ", 12016\n",
      "J 11289\n",
      ". 8039\n",
      "C 7735\n",
      ": 5627\n",
      "T 5181\n",
      "M 4410\n",
      "' 2557\n",
      "W 2556\n",
      "` 1847\n",
      "E 456\n",
      "U 362\n",
      "( 107\n",
      ") 107\n",
      "F 22\n",
      "$ 1\n"
     ]
    }
   ],
   "source": [
    "# Print the distribution of the POS tags\n",
    "\n",
    "tagclasses = nltk.FreqDist(tag[0] for (word, tag) in emmatagged)\n",
    "for tag, freq in tagclasses.most_common():\n",
    "    print(tag, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her. \n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the text into sentences\n",
    "\n",
    "textsplit = nltk.sent_tokenize(emmatext)\n",
    "print(textsplit[0], \"\\n\")\n",
    "print(textsplit[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.'] \n",
      "\n",
      "['She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the list of sentences\n",
    "\n",
    "tokentext = [nltk.word_tokenize(sent) for sent in textsplit]\n",
    "print(tokentext[0], \"\\n\")\n",
    "print(tokentext[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'NNS'), ('Emma', 'NNP'), ('by', 'IN'), ('Jane', 'NNP'), ('Austen', 'NNP'), ('1816', 'CD'), (']', 'NNP'), ('VOLUME', 'NNP'), ('I', 'PRP'), ('CHAPTER', 'VBP'), ('I', 'PRP'), ('Emma', 'NNP'), ('Woodhouse', 'NNP'), (',', ','), ('handsome', 'NN'), (',', ','), ('clever', 'NN'), (',', ','), ('and', 'CC'), ('rich', 'JJ'), (',', ','), ('with', 'IN'), ('a', 'DT'), ('comfortable', 'JJ'), ('home', 'NN'), ('and', 'CC'), ('happy', 'JJ'), ('disposition', 'NN'), (',', ','), ('seemed', 'VBD'), ('to', 'TO'), ('unite', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('blessings', 'NNS'), ('of', 'IN'), ('existence', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), ('lived', 'VBN'), ('nearly', 'RB'), ('twenty-one', 'CD'), ('years', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('with', 'IN'), ('very', 'RB'), ('little', 'JJ'), ('to', 'TO'), ('distress', 'VB'), ('or', 'CC'), ('vex', 'VB'), ('her', 'PRP'), ('.', '.')] \n",
      "\n",
      "[('She', 'PRP'), ('was', 'VBD'), ('the', 'DT'), ('youngest', 'JJS'), ('of', 'IN'), ('the', 'DT'), ('two', 'CD'), ('daughters', 'NNS'), ('of', 'IN'), ('a', 'DT'), ('most', 'RBS'), ('affectionate', 'JJ'), (',', ','), ('indulgent', 'JJ'), ('father', 'NN'), (';', ':'), ('and', 'CC'), ('had', 'VBD'), (',', ','), ('in', 'IN'), ('consequence', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sister', 'NN'), (\"'s\", 'POS'), ('marriage', 'NN'), (',', ','), ('been', 'VBN'), ('mistress', 'NN'), ('of', 'IN'), ('his', 'PRP$'), ('house', 'NN'), ('from', 'IN'), ('a', 'DT'), ('very', 'RB'), ('early', 'JJ'), ('period', 'NN'), ('.', '.')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the tagger on the list of sentences\n",
    "\n",
    "taggedtext = [nltk.pos_tag(tokens) for tokens in tokentext]\n",
    "print(taggedtext[0], \"\\n\")\n",
    "print(taggedtext[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[', 'NNS') \n",
      "\n",
      "('Emma', 'NNP') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list to get just one list of (word, tag)\n",
    "\n",
    "taggedtextflat = [pair for sent in taggedtext for pair in sent]\n",
    "print(taggedtextflat[0], \"\\n\")\n",
    "print(taggedtextflat[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N 31998\n",
      "V 31297\n",
      "P 21612\n",
      "I 17880\n",
      "R 14008\n",
      "D 12743\n",
      ", 12016\n",
      "J 11227\n",
      ". 8041\n",
      "C 7724\n",
      ": 5627\n",
      "T 5181\n",
      "M 4426\n",
      "W 2570\n",
      "' 2558\n",
      "` 1847\n",
      "E 456\n",
      "U 344\n",
      "( 107\n",
      ") 107\n",
      "F 17\n",
      "$ 1\n"
     ]
    }
   ],
   "source": [
    "# Print the distribution of the POS tags\n",
    "\n",
    "tagclasses = nltk.FreqDist(tag[0] for (word, tag) in taggedtextflat)\n",
    "for tag, freq in tagclasses.most_common():\n",
    "    print(tag, freq)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "837a5e7ae28b4e198c1d8cf4796a058d56e0b65e3e1483208ccae61f9a209f4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
