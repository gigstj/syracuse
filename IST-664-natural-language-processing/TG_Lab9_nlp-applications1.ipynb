{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk\n",
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## repeat the setup of the movie review sentences for classification\n",
    "# for each sentence(document), get its words and category (positive/negative)\n",
    "documents = [(sent, cat) for cat in nltk.corpus.sentence_polarity.categories() \n",
    "    for sent in nltk.corpus.sentence_polarity.sents(categories = cat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the documents\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21401\n"
     ]
    }
   ],
   "source": [
    "# get all words from all movie_reviews and put into a frequency distribution\n",
    "#   note lowercase, but no stemming or stopwords\n",
    "all_words_list = [word for (sent, cat) in documents for word in sent]\n",
    "all_words = nltk.FreqDist(all_words_list)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 1500 most frequently appearing keywords in the corpus\n",
    "word_items = all_words.most_common(1500)\n",
    "word_features = [word for (word,count) in word_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features (keywords) of a document for a BOW/unigram baseline\n",
    "# each feature is 'contains(keyword)' and is true or false depending\n",
    "# on whether that keyword is in the document\n",
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features sets for a document, including keyword features and category feature\n",
    "featuresets = [(document_features(d, word_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training using naive Baysian classifier, training set is 90% of data\n",
    "train_set, test_set = featuresets[1000:], featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.733"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the accuracy of the classifier\n",
    "# the accuracy result may vary since we randomized the documents\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####   adding Bigram features   ####\n",
    "# set up for using bigrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['more', 'vaudeville', 'show', 'than', 'well-constructed', 'narrative', ',', 'but', 'on', 'those', 'terms', \"it's\", 'inoffensive', 'and', 'actually', 'rather', 'sweet', '.', 'jones', 'has', 'delivered', 'a', 'solidly', 'entertaining', 'and', 'moving', 'family', 'drama', '.', 'a', 'muddle', 'splashed', 'with', 'bloody', 'beauty', 'as', 'vivid', 'as', 'any', 'scorsese', 'has', 'ever', 'given', 'us', '.', 'by', 'the', 'end', ',', 'i']\n"
     ]
    }
   ],
   "source": [
    "# create the bigram finder on all the words in sequence\n",
    "print(all_words_list[:50])\n",
    "finder = BigramCollocationFinder.from_words(all_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"''independent\", \"film''\"), (\"'60s-homage\", 'pokepie'), (\"'[the\", 'cockettes]'), (\"'ace\", \"ventura'\"), (\"'alternate\", \"reality'\"), (\"'aunque\", 'recurre'), (\"'black\", \"culture'\"), (\"'blue\", \"crush'\"), (\"'chan\", \"moment'\"), (\"'chick\", \"flicks'\"), (\"'date\", \"movie'\"), (\"'ethnic\", 'cleansing'), (\"'face\", \"value'\"), (\"'fully\", \"experienced'\"), (\"'jason\", \"x'\"), (\"'juvenile\", \"delinquent'\"), (\"'laugh\", \"therapy'\"), (\"'masterpiece\", \"theatre'\"), (\"'nicholas\", \"nickleby'\"), (\"'old\", \"neighborhood'\"), (\"'opening\", \"up'\"), (\"'rare\", \"birds'\"), (\"'sacre\", 'bleu'), (\"'science\", \"fiction'\"), (\"'shindler's\", \"list'\"), (\"'snow\", \"dogs'\"), (\"'some\", \"body'\"), (\"'special\", \"effects'\"), (\"'terrible\", \"filmmaking'\"), (\"'time\", \"waster'\"), (\"'true\", \"story'\"), (\"'unfaithful'\", 'cheats'), (\"'very\", \"sneaky'\"), (\"'we're\", '-doing-it-for'), (\"'who's\", \"who'\"), ('-after', 'spangle'), ('-as-it-', 'thinks-it-is'), ('-as-nasty', '-as-it-'), ('-doing-it-for', \"-the-cash'\"), ('10-course', 'banquet'), ('10-year', 'delay'), ('15-cent', 'stump'), ('18-year-old', 'mistress'), (\"1950's\", 'doris'), (\"1983's\", 'koyaanisqatsi'), ('1986', 'harlem'), (\"1988's\", 'powaqqatsi'), ('1992', 'malfitano-domingo'), (\"1992's\", 'unforgiven'), ('22-year-old', 'girlfriend')]\n"
     ]
    }
   ],
   "source": [
    "# define the top 500 bigrams using the chi squared measure\n",
    "bigram_features = finder.nbest(bigram_measures.chi_sq, 500)\n",
    "print(bigram_features[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Arthur', 'carefully'), ('carefully', 'rode'), ('rode', 'the'), ('the', 'brown'), ('brown', 'horse'), ('horse', 'around'), ('around', 'the'), ('the', 'castle')]\n"
     ]
    }
   ],
   "source": [
    "# examples to demonstrate the bigram feature function definition\n",
    "sent = ['Arthur','carefully','rode','the','brown','horse','around','the','castle']\n",
    "sentbigrams = list(nltk.bigrams(sent))\n",
    "print(sentbigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "B_brown_horse\n"
     ]
    }
   ],
   "source": [
    "# for a single bigram, test if it's in the sentence bigrams and format the feature name\n",
    "bigram = ('brown','horse')\n",
    "print(bigram in sentbigrams)\n",
    "print('B_{}_{}'.format(bigram[0], bigram[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features that include words as before \n",
    "# add the most frequent significant bigrams\n",
    "# this function takes the list of words in a document as an argument and returns a feature dictionary\n",
    "# it depends on the variables word_features and bigram_features\n",
    "def bigram_document_features(document, word_features, bigram_features):\n",
    "    document_words = set(document)\n",
    "    document_bigrams = nltk.bigrams(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    for bigram in bigram_features:\n",
    "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this function to create feature sets for all sentences\n",
    "bigram_featuresets = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "# number of features for document 0\n",
    "print(len(bigram_featuresets[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features in document 0\n",
    "# print(bigram_featuresets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.733"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a classifier and report accuracy\n",
    "train_set, test_set = bigram_featuresets[1000:], bigram_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Arthur', 'carefully', 'rode', 'the', 'brown', 'horse', 'around', 'the', 'castle']\n",
      "[('Arthur', 'NNP'), ('carefully', 'RB'), ('rode', 'VBD'), ('the', 'DT'), ('brown', 'JJ'), ('horse', 'NN'), ('around', 'IN'), ('the', 'DT'), ('castle', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "###  POS tag counts\n",
    "# using the default pos tagger in NLTK (the Stanford tagger)\n",
    "print(sent)\n",
    "print(nltk.pos_tag(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes a document list of words and returns a feature dictionary\n",
    "# it runs the default pos tagger (the Stanford tagger) on the document\n",
    "#   and counts 4 types of pos tags to use as features\n",
    "def POS_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    tagged_words = nltk.pos_tag(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    numNoun = 0\n",
    "    numVerb = 0\n",
    "    numAdj = 0\n",
    "    numAdverb = 0\n",
    "    for (word, tag) in tagged_words:\n",
    "        if tag.startswith('N'): numNoun += 1\n",
    "        if tag.startswith('V'): numVerb += 1\n",
    "        if tag.startswith('J'): numAdj += 1\n",
    "        if tag.startswith('R'): numAdverb += 1\n",
    "    features['nouns'] = numNoun\n",
    "    features['verbs'] = numVerb\n",
    "    features['adjectives'] = numAdj\n",
    "    features['adverbs'] = numAdverb\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504\n"
     ]
    }
   ],
   "source": [
    "# define feature sets using this function\n",
    "POS_featuresets = [(POS_features(d, word_features), c) for (d, c) in documents]\n",
    "# number of features for document 0\n",
    "print(len(POS_featuresets[0][0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['more', 'vaudeville', 'show', 'than', 'well-constructed', 'narrative', ',', 'but', 'on', 'those', 'terms', \"it's\", 'inoffensive', 'and', 'actually', 'rather', 'sweet', '.'], 'pos')\n",
      "num nouns 2\n",
      "num verbs 2\n",
      "num adjectives 4\n",
      "num adverbs 3\n"
     ]
    }
   ],
   "source": [
    "# the first sentence\n",
    "print(documents[0])\n",
    "# the pos tag features for this sentence\n",
    "print('num nouns', POS_featuresets[0][0]['nouns'])\n",
    "print('num verbs', POS_featuresets[0][0]['verbs'])\n",
    "print('num adjectives', POS_featuresets[0][0]['adjectives'])\n",
    "print('num adverbs', POS_featuresets[0][0]['adverbs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.732"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train and test the classifier\n",
    "train_set, test_set = POS_featuresets[1000:], POS_featuresets[:1000]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross-validation ##\n",
    "# this function takes the number of folds, the feature sets\n",
    "# it iterates over the folds, using different sections for training and testing in turn\n",
    "#   it prints the accuracy for each fold and the average accuracy at the end\n",
    "def cross_validation_accuracy(num_folds, featuresets):\n",
    "    subset_size = int(len(featuresets)/num_folds)\n",
    "    print('Each fold size:', subset_size)\n",
    "    accuracy_list = []\n",
    "    # iterate over the folds\n",
    "    for i in range(num_folds):\n",
    "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
    "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
    "        # train using train_this_round\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
    "        # evaluate against test_this_round and save accuracy\n",
    "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)\n",
    "        print (i, accuracy_this_round)\n",
    "        accuracy_list.append(accuracy_this_round)\n",
    "    # find mean accuracy over all rounds\n",
    "    print ('mean accuracy', sum(accuracy_list) / num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each fold size: 2132\n",
      "0 0.7457786116322702\n",
      "1 0.7307692307692307\n",
      "2 0.7457786116322702\n",
      "3 0.7335834896810507\n",
      "4 0.7317073170731707\n",
      "mean accuracy 0.7375234521575985\n"
     ]
    }
   ],
   "source": [
    "# perform the cross-validation on the featuresets with word features and generate accuracy\n",
    "num_folds = 5\n",
    "cross_validation_accuracy(num_folds, featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos', 'pos', 'pos', 'neg', 'neg', 'pos', 'pos', 'pos', 'pos', 'neg', 'neg', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'pos', 'pos', 'pos', 'pos', 'neg', 'pos', 'neg', 'neg', 'pos', 'pos', 'pos', 'neg', 'pos']\n",
      "['neg', 'pos', 'pos', 'neg', 'pos', 'pos', 'neg', 'pos', 'neg', 'pos', 'neg', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg', 'pos', 'pos', 'pos', 'pos', 'neg', 'pos', 'pos', 'neg', 'neg', 'pos', 'neg', 'neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "## other evaluation measures:  confusion matrix, precision, recall, F1 ##\n",
    "\n",
    "goldlist = []\n",
    "predictedlist = []\n",
    "for (features, label) in test_set:\n",
    "    \tgoldlist.append(label)\n",
    "    \tpredictedlist.append(classifier.classify(features))\n",
    "\n",
    "# look at the first 30 examples\n",
    "print(goldlist[:30])\n",
    "print(predictedlist[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   n   p |\n",
      "    |   e   o |\n",
      "    |   g   s |\n",
      "----+---------+\n",
      "neg |<379>136 |\n",
      "pos | 132<353>|\n",
      "----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = nltk.ConfusionMatrix(goldlist, predictedlist)\n",
    "print(cm.pretty_format(sort_by_count=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |      n      p |\n",
      "    |      e      o |\n",
      "    |      g      s |\n",
      "----+---------------+\n",
      "neg | <37.9%> 13.6% |\n",
      "pos |  13.2% <35.3%>|\n",
      "----+---------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# or show the results as percentages\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute precision, recall and F1 for each label\n",
    "#  and for any number of labels\n",
    "# Input: list of gold labels, list of predicted labels (in same order)\n",
    "# Output:  prints precision, recall and F1 for each label\n",
    "def eval_measures(gold, predicted):\n",
    "    # get a list of labels\n",
    "    labels = list(set(gold))\n",
    "    # these lists have values for each label \n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    F1_list = []\n",
    "    for lab in labels:\n",
    "        # for each label, compare gold and predicted lists and compute values\n",
    "        TP = FP = FN = TN = 0\n",
    "        for i, val in enumerate(gold):\n",
    "            if val == lab and predicted[i] == lab:  TP += 1\n",
    "            if val == lab and predicted[i] != lab:  FN += 1\n",
    "            if val != lab and predicted[i] == lab:  FP += 1\n",
    "            if val != lab and predicted[i] != lab:  TN += 1\n",
    "        # use these to compute recall, precision, F1\n",
    "        recall = TP / (TP + FP)\n",
    "        precision = TP / (TP + FN)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        F1_list.append( 2 * (recall * precision) / (recall + precision))\n",
    "\n",
    "    # the evaluation measures in a table with one row per label\n",
    "    print('\\tPrecision\\tRecall\\t\\tF1')\n",
    "    # print measures for each label\n",
    "    for i, lab in enumerate(labels):\n",
    "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
    "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(F1_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPrecision\tRecall\t\tF1\n",
      "neg \t      0.736      0.742      0.739\n",
      "pos \t      0.728      0.722      0.725\n"
     ]
    }
   ],
   "source": [
    "# call the function with our data\n",
    "eval_measures(goldlist, predictedlist)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "837a5e7ae28b4e198c1d8cf4796a058d56e0b65e3e1483208ccae61f9a209f4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
