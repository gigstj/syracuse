{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<font size = \"4\">\n",
    "\n",
    "The United States Congress is a series of legislative debates, during which, there are a number of topics discussed by politicians. It is believed by political scientists that each congress typically runs around 40-50 or so topics. In this analysis, I will be conducting LDA topic modeling on text data related to the 110th Congress. The 110th Congress took place between 2007 to 2009. The data I will be using consists of all of the captured text from what the politicians discussed during the 110th Congress. It consists of the House of Representatives and does not consist of the Senate.\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas\n",
    "import numpy\n",
    "import re\n",
    "import os\n",
    "import bs4\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy\n",
    "import smart_open\n",
    "import gensim\n",
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pyLDAvis.sklearn as LDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lilgi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Additional setup\n",
    "nltk.download(\"wordnet\")\n",
    "numpy.random.seed(2018)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "<font size = \"4\">\n",
    "\n",
    "In the following code, I will load the text data. The data comes in the form of a top level folder, with four subfolders, and within each subfolder consists a number of text files associated with individual house speakers. The subfolders are categorized by female democrats, female republicans, male democrats, and male republicans. I will read in the text by iterating through each of the files, using the Beautiful Soup package to parse the text, and then save that in a dataframe.\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the paths for the 4 directories within the 110 directory\n",
    "dir1 = \"110/110-f-d\"\n",
    "dir2 = \"110/110-f-r\"\n",
    "dir3 = \"110/110-m-d\"\n",
    "dir4 = \"110/110-m-r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load the text files\n",
    "def load_files(dir):\n",
    "\n",
    "    # Save the column names for the subsequent dataframe\n",
    "    columns = [\"FileName\", \"FilePath\", \"DocNumber\", \"Text\"]\n",
    "\n",
    "    # Initiate a dataframe called master which will store the data\n",
    "    master = pandas.DataFrame(columns = columns)\n",
    "\n",
    "    # Iterate through each file contained in the directory\n",
    "    for file in os.listdir(dir):\n",
    "\n",
    "        # Save the file name - will become a col in the df\n",
    "        file_name = file\n",
    "\n",
    "        # Save the file path - will become a col in the df\n",
    "        file_path = dir + \"/\" + file\n",
    "\n",
    "        # Initiate a counter to keep track of how many documents there are\n",
    "        doc_counter = 0\n",
    "\n",
    "        # Access the contents of the file\n",
    "        soup = bs4.BeautifulSoup(open(file_path))\n",
    "\n",
    "        # Iterate through all of the text tags in the html\n",
    "        for doc in soup.find_all(\"text\"):\n",
    "\n",
    "            # Retrieve the text and append it to the list\n",
    "            text = doc.get_text()\n",
    "\n",
    "            # Save the file name, file path, doc number, and text in a temp dictionary\n",
    "            temp = {\n",
    "                \"FileName\": file_name,\n",
    "                \"FilePath\": file_path,\n",
    "                \"DocNumber\": doc_counter + 1,\n",
    "                \"Text\": text\n",
    "            }\n",
    "\n",
    "            # Append the dictionary to the master dataframe\n",
    "            master = master.append(temp, ignore_index = True)\n",
    "\n",
    "            # Add one to the counter after each iteration\n",
    "            doc_counter += 1\n",
    "\n",
    "    # Return the master dataframe\n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text files\n",
    "fd = load_files(dir1)   # Female and democrat\n",
    "fr = load_files(dir2)   # Female and republican\n",
    "md = load_files(dir3)   # Male and democrat\n",
    "mr = load_files(dir4)   # Male and republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for the sex and the political party\n",
    "fd[\"Sex\"] = 0; fd[\"Party\"] = 0\n",
    "fr[\"Sex\"] = 0; fr[\"Party\"] = 1\n",
    "md[\"Sex\"] = 1; md[\"Party\"] = 0\n",
    "mr[\"Sex\"] = 1; mr[\"Party\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the data into one large dataframe\n",
    "congress = pandas.concat([fd, fr, md, mr]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "<font size = \"4\">\n",
    "\n",
    "Now that the data has been loaded, I will display the first 5 observations of the dataframe, as well as an example of what one of the text documents look liks. I will also take a look at the distribution of the length of each text document.\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>FilePath</th>\n",
       "      <th>DocNumber</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110_baldwin_x_wi.txt</td>\n",
       "      <td>110/110-f-d/110_baldwin_x_wi.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n   Ms. BALDWIN. Mr. Speaker, I yield myself ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110_baldwin_x_wi.txt</td>\n",
       "      <td>110/110-f-d/110_baldwin_x_wi.txt</td>\n",
       "      <td>2</td>\n",
       "      <td>\\n   Ms. BALDWIN. Mr. Speaker, I would also li...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110_baldwin_x_wi.txt</td>\n",
       "      <td>110/110-f-d/110_baldwin_x_wi.txt</td>\n",
       "      <td>3</td>\n",
       "      <td>\\n   Ms. BALDWIN. Mr. Speaker, I yield myself ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110_baldwin_x_wi.txt</td>\n",
       "      <td>110/110-f-d/110_baldwin_x_wi.txt</td>\n",
       "      <td>4</td>\n",
       "      <td>\\n   Ms. BALDWIN. Mr. Speaker, the Loving v. V...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110_baldwin_x_wi.txt</td>\n",
       "      <td>110/110-f-d/110_baldwin_x_wi.txt</td>\n",
       "      <td>5</td>\n",
       "      <td>\\n   Ms. BALDWIN. Mr. Speaker, the matter befo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               FileName                          FilePath DocNumber  \\\n",
       "0  110_baldwin_x_wi.txt  110/110-f-d/110_baldwin_x_wi.txt         1   \n",
       "1  110_baldwin_x_wi.txt  110/110-f-d/110_baldwin_x_wi.txt         2   \n",
       "2  110_baldwin_x_wi.txt  110/110-f-d/110_baldwin_x_wi.txt         3   \n",
       "3  110_baldwin_x_wi.txt  110/110-f-d/110_baldwin_x_wi.txt         4   \n",
       "4  110_baldwin_x_wi.txt  110/110-f-d/110_baldwin_x_wi.txt         5   \n",
       "\n",
       "                                                Text  Sex  Party  \n",
       "0  \\n   Ms. BALDWIN. Mr. Speaker, I yield myself ...    0      0  \n",
       "1  \\n   Ms. BALDWIN. Mr. Speaker, I would also li...    0      0  \n",
       "2  \\n   Ms. BALDWIN. Mr. Speaker, I yield myself ...    0      0  \n",
       "3  \\n   Ms. BALDWIN. Mr. Speaker, the Loving v. V...    0      0  \n",
       "4  \\n   Ms. BALDWIN. Mr. Speaker, the matter befo...    0      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top 5 observations in the dataframe\n",
    "congress.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Ms. BALDWIN. Mr. Speaker, I yield myself such time as I may consume. \n",
      "   As part of our Nation's bicentennial celebration in 1976, Congress passed a joint resolution re-emphasizing existing rules and customs pertaining to the display and use of the flag, especially recommending its display on a number of different holidays, including Mother's Day, the second Sunday in May. \n",
      "   Omitted from the list was Father's Day. H.R. 2356 would amend the Federal flag code to include Father's Day, the third Sunday in June, among important holidays on which to fly the American flag. \n",
      "   The law now provides that, in addition to the important occasions listed in the flag code, ``the flag should be displayed on all days.'' I know that this is the custom in every community in the United States. \n",
      "   Still, I think that it is important for the flag code to recognize both mothers and fathers, who raise the next generation, inculcate them with the values they need to be good citizens and good neighbors. \n",
      "   I want to thank our colleague, the gentleman from Georgia (Mr. Scott) for his efforts to enact this worthwhile legislation. \n",
      "   And I urge my colleagues to join me in supporting this legislation to honor fathers in the flag code, just as we now honor mothers. \n",
      "   Mr. Speaker, I reserve the balance of my time. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the text from the first observation\n",
    "print(congress.Text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0\n",
      "count  40433.000000\n",
      "mean     465.201568\n",
      "std      648.661479\n",
      "min       53.000000\n",
      "25%      182.000000\n",
      "50%      314.000000\n",
      "75%      568.000000\n",
      "max    17177.000000 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show summary statistics and a distribution of the length of text from each file\n",
    "text_lengths = []\n",
    "for text in congress.Text:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    text_lengths.append(len(tokens))\n",
    "print(pandas.DataFrame(text_lengths).describe(), \"\\n\")\n",
    "matplotlib.pyplot.hist(text_lengths, bins = 100)\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "<font size = \"4\">\n",
    "\n",
    "There are a large number of text documents - about 40,000. With 429 different individuals from the House of Representatives. That means on average, each representative had about 100 documents or so. The lengths of the documents are pretty closely distributed with some outliers.\n",
    "\n",
    "In the next section, I will go through the LDA process using the Gensim package. I will first preprocess the text, and then fit the LDA model, followed by using the pyLDAvis package to construct an interactive HTML document that will allow me to further analyze the results form the LDA model.\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df with just the text\n",
    "documents = pandas.DataFrame(\n",
    "    list(zip(congress.index, congress[\"Text\"])),\n",
    "    columns = [\"Index\", \"Text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform preprocessing\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [baldwin, speaker, yield, time, consume, natio...\n",
      "1    [baldwin, speaker, like, join, gentleman, iowa...\n",
      "2    [baldwin, speaker, yield, time, consume, speak...\n",
      "3    [baldwin, speaker, loving, virginia, decision,...\n",
      "4    [baldwin, speaker, matter, today, iraq, supple...\n",
      "5    [baldwin, speaker, rise, support, resolution, ...\n",
      "6    [baldwin, speaker, thank, chairman, filner, ri...\n",
      "7    [baldwin, speaker, rise, today, colleagues, ho...\n",
      "8    [baldwin, speaker, thank, chairman, speaker, r...\n",
      "9    [baldwin, madam, chairman, yield, minutes, mad...\n",
      "Name: Text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text, saving the results as \"processed_docs\"\n",
    "processed_docs = documents[\"Text\"].map(preprocess)\n",
    "print(processed_docs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary from \"processed_docs\" containing the number of\n",
    "# times a word appears in the data\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 addition\n",
      "1 amend\n",
      "2 american\n",
      "3 balance\n",
      "4 baldwin\n",
      "5 bicentennial\n",
      "6 celebration\n",
      "7 citizens\n",
      "8 code\n",
      "9 colleague\n",
      "10 colleagues\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 10 words\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that appear in less than 15 docs or than .5 docs (fraction of corpus size)\n",
    "# After the two above steps, keep only the first 100000 most frequent tokens\n",
    "dictionary.filter_extremes(no_below = 100, no_above = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each document create a dictionary reporting how many words and how many times the words appear\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.057490631559960426),\n",
      " (1, 0.08152381440458328),\n",
      " (2, 0.02232151319377536),\n",
      " (3, 0.03340282020001905),\n",
      " (4, 0.11507454591242622),\n",
      " (5, 0.09739478633777479),\n",
      " (6, 0.05211958212128136),\n",
      " (7, 0.3307402588046778),\n",
      " (8, 0.04388419975362708),\n",
      " (9, 0.023644406927892093),\n",
      " (10, 0.04374747122285954),\n",
      " (11, 0.020144855453864297),\n",
      " (12, 0.045012953801724695),\n",
      " (13, 0.11078681889373727),\n",
      " (14, 0.05001195573166657),\n",
      " (15, 0.0529577467364102),\n",
      " (16, 0.2244112361285344),\n",
      " (17, 0.1147684591357682),\n",
      " (18, 0.04593757610494175),\n",
      " (19, 0.09432257873088964),\n",
      " (20, 0.05463333940245981),\n",
      " (21, 0.06833401660148843),\n",
      " (22, 0.14866525871859806),\n",
      " (23, 0.17956661304212818),\n",
      " (24, 0.033690871565876024),\n",
      " (25, 0.6872319128369573),\n",
      " (26, 0.06901988603428989),\n",
      " (27, 0.025945373069794764),\n",
      " (28, 0.05894686354662444),\n",
      " (29, 0.05989597890545781),\n",
      " (30, 0.10286158990953516),\n",
      " (31, 0.07683520604134522),\n",
      " (32, 0.05866737536172525),\n",
      " (33, 0.04328257499800945),\n",
      " (34, 0.0480885911873588),\n",
      " (35, 0.08305634002358629),\n",
      " (36, 0.07435284952686447),\n",
      " (37, 0.024681918657096284),\n",
      " (38, 0.05130181914756549),\n",
      " (39, 0.06979441390800319),\n",
      " (40, 0.10604107426741283),\n",
      " (41, 0.07705595356371199),\n",
      " (42, 0.18853575333146766),\n",
      " (43, 0.029697794025407738),\n",
      " (44, 0.02516143010519946),\n",
      " (45, 0.0836264721242054),\n",
      " (46, 0.03890536327040127),\n",
      " (47, 0.09510450547438991),\n",
      " (48, 0.039812872546890136),\n",
      " (49, 0.05032682517898347),\n",
      " (50, 0.060557838636100945),\n",
      " (51, 0.05259228189386557),\n",
      " (52, 0.040831711802041565),\n",
      " (53, 0.04047013215972027),\n",
      " (54, 0.04617927331881727),\n",
      " (55, 0.0814372125660698),\n",
      " (56, 0.05023548882765032),\n",
      " (57, 0.017487779211631235),\n",
      " (58, 0.024960668572989824),\n",
      " (59, 0.19556316430186407),\n",
      " (60, 0.057342865443153715),\n",
      " (61, 0.021419700725650158),\n",
      " (62, 0.02549997621597519),\n",
      " (63, 0.025998014749265),\n",
      " (64, 0.029081403500549458),\n",
      " (65, 0.03036302739846039),\n",
      " (66, 0.07338528961059716),\n",
      " (67, 0.021545659010814046),\n",
      " (68, 0.11194007385843614),\n",
      " (69, 0.02729577322798354)]\n"
     ]
    }
   ],
   "source": [
    "# Create tf-idf model object using models.TfidfModel on \"bow_corpus\"\n",
    "# Save it to \"tfidf\" then apply transformation to the entire corpus and\n",
    "# call it \"corpus_tfidf\". Finally we preview TF-IDF scores for our first document\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "for doc in corpus_tfidf: pprint(doc); break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the lda model\n",
    "lda_model = LdaMulticore(bow_corpus, num_topics = 15, id2word = dictionary, passes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -7.332847666941751\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "# a measure of how good the model is. lower the better.\n",
    "perplx = lda_model.log_perplexity(bow_corpus)\n",
    "print('\\nPerplexity: ', perplx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.029*\"going\" + 0.028*\"people\" + 0.020*\"think\" + 0.017*\"know\" + '\n",
      "  '0.015*\"want\" + 0.012*\"said\" + 0.009*\"things\" + 0.009*\"come\" + '\n",
      "  '0.009*\"country\" + 0.009*\"need\"'),\n",
      " (1,\n",
      "  '0.051*\"health\" + 0.042*\"care\" + 0.035*\"children\" + 0.016*\"insurance\" + '\n",
      "  '0.015*\"program\" + 0.012*\"medicare\" + 0.009*\"medical\" + 0.008*\"speaker\" + '\n",
      "  '0.008*\"percent\" + 0.008*\"schip\"'),\n",
      " (2,\n",
      "  '0.009*\"economy\" + 0.009*\"financial\" + 0.008*\"families\" + 0.008*\"american\" + '\n",
      "  '0.008*\"today\" + 0.008*\"workers\" + 0.007*\"credit\" + 0.007*\"legislation\" + '\n",
      "  '0.007*\"market\" + 0.007*\"speaker\"'),\n",
      " (3,\n",
      "  '0.050*\"energy\" + 0.012*\"percent\" + 0.012*\"prices\" + 0.010*\"american\" + '\n",
      "  '0.009*\"natural\" + 0.009*\"america\" + 0.008*\"trade\" + 0.008*\"world\" + '\n",
      "  '0.008*\"fuel\" + 0.008*\"today\"'),\n",
      " (4,\n",
      "  '0.014*\"speaker\" + 0.011*\"veterans\" + 0.009*\"service\" + 0.009*\"time\" + '\n",
      "  '0.009*\"today\" + 0.008*\"great\" + 0.008*\"thank\" + 0.008*\"honor\" + '\n",
      "  '0.008*\"nation\" + 0.007*\"years\"'),\n",
      " (5,\n",
      "  '0.015*\"united\" + 0.014*\"states\" + 0.011*\"people\" + 0.011*\"speaker\" + '\n",
      "  '0.010*\"rights\" + 0.009*\"world\" + 0.009*\"government\" + 0.007*\"america\" + '\n",
      "  '0.006*\"human\" + 0.006*\"american\"'),\n",
      " (6,\n",
      "  '0.016*\"federal\" + 0.012*\"border\" + 0.011*\"state\" + 0.009*\"national\" + '\n",
      "  '0.009*\"water\" + 0.009*\"states\" + 0.008*\"speaker\" + 0.007*\"local\" + '\n",
      "  '0.007*\"enforcement\" + 0.007*\"legislation\"'),\n",
      " (7,\n",
      "  '0.028*\"budget\" + 0.020*\"spending\" + 0.018*\"money\" + 0.017*\"billion\" + '\n",
      "  '0.014*\"year\" + 0.014*\"taxes\" + 0.013*\"increase\" + 0.011*\"american\" + '\n",
      "  '0.011*\"government\" + 0.009*\"percent\"'),\n",
      " (8,\n",
      "  '0.037*\"research\" + 0.018*\"stem\" + 0.014*\"cell\" + 0.012*\"support\" + '\n",
      "  '0.012*\"disease\" + 0.012*\"speaker\" + 0.011*\"life\" + 0.010*\"human\" + '\n",
      "  '0.009*\"cells\" + 0.009*\"cancer\"'),\n",
      " (9,\n",
      "  '0.015*\"security\" + 0.013*\"chairman\" + 0.010*\"amendment\" + 0.009*\"national\" '\n",
      "  '+ 0.008*\"legislation\" + 0.008*\"department\" + 0.007*\"support\" + '\n",
      "  '0.007*\"committee\" + 0.007*\"program\" + 0.006*\"important\"'),\n",
      " (10,\n",
      "  '0.042*\"iraq\" + 0.021*\"troops\" + 0.019*\"president\" + 0.014*\"military\" + '\n",
      "  '0.011*\"speaker\" + 0.009*\"american\" + 0.009*\"congress\" + 0.008*\"iraqi\" + '\n",
      "  '0.007*\"support\" + 0.007*\"people\"'),\n",
      " (11,\n",
      "  '0.025*\"education\" + 0.020*\"students\" + 0.020*\"school\" + 0.018*\"college\" + '\n",
      "  '0.012*\"university\" + 0.011*\"speaker\" + 0.010*\"schools\" + 0.009*\"student\" + '\n",
      "  '0.009*\"national\" + 0.008*\"state\"'),\n",
      " (12,\n",
      "  '0.022*\"program\" + 0.014*\"programs\" + 0.012*\"funding\" + 0.012*\"support\" + '\n",
      "  '0.012*\"housing\" + 0.011*\"chairman\" + 0.010*\"million\" + 0.009*\"small\" + '\n",
      "  '0.009*\"help\" + 0.008*\"assistance\"'),\n",
      " (13,\n",
      "  '0.022*\"committee\" + 0.019*\"speaker\" + 0.018*\"chairman\" + 0.017*\"time\" + '\n",
      "  '0.017*\"amendment\" + 0.016*\"gentleman\" + 0.015*\"house\" + 0.011*\"yield\" + '\n",
      "  '0.010*\"members\" + 0.009*\"legislation\"'),\n",
      " (14,\n",
      "  '0.036*\"speaker\" + 0.029*\"house\" + 0.021*\"vote\" + 0.019*\"states\" + '\n",
      "  '0.019*\"resolution\" + 0.017*\"district\" + 0.014*\"congress\" + 0.014*\"title\" + '\n",
      "  '0.012*\"state\" + 0.012*\"mental\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a vis to an html document for further interpretation\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)\n",
    "pyLDAvis.save_html(vis, \"lda_example.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>&nbsp;</p>\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<font size = \"4\">\n",
    "\n",
    "After some trial and error, I found that the best result came from modeling about 20 topics, based on the perplexity score and based on my own judgement in reviewing the topics. When I tried to model 40 topics, there was too much overlap between the topics and I could tell that some words were not put in the right topic. I also tried to model 10 and 15 topics, but I felt like there were not enough topics in this case because some of the topics had words seemed like they could have been further broken down.\n",
    "\n",
    "Out of the 20 topics, here are the ones that I think are the most distinguished - Spend, Government, Iraq, Socioeconomic, Healthcare, Science, Border Control, Education, and Agriculture. It seemed like the words associated with these topics made the most sense, but some of the other topics I could not come up with a topic. I would be interested to see if there were any real labels that could be found for this dataset, but I do not think they are, which is why this is an unsupervised problem.\n",
    "\n",
    "<p>&nbsp;</p>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "837a5e7ae28b4e198c1d8cf4796a058d56e0b65e3e1483208ccae61f9a209f4c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
